---
title: '1. Evalute your chat flow'
layout: default
nav_order: 1
parent: 'Exercise 03: Evaluating and Deploying LLMs'
---

# Task 01 - Add to your existing Prompt Flow three evaluation flows

## Description

In this task, you will reuse the Prompt Flow you worked on in the previous exercise, add a new output and generate 3 new evaluation flows (Groundedness, Similarity and Relevance) to batch run and evaluate the execution based on sample data provided.

## Success Criteria

* Verify that the new output is added to the Prompt Flow.
* Verify that your 3 new evaluation flows are created and ready to be executed.
* Verify that you can run the evaluation and see the results in List View and Dashboard view.

## Description of Evaluation Flows

Azure AI Studio provides practitioners with tools for manual and automated evaluation that can help you with the measurement stage. We recommend that you start with manual evaluation then proceed to automated evaluation. Manual evaluation, that is, manually reviewing the applicationâ€™s generated outputs, is useful for tracking progress on a small set of priority issues. When mitigating specific risks, it's often most productive to keep manually checking progress against a small dataset until evidence of the risks is no longer observed before moving to automated evaluation. Azure AI Studio supports a manual evaluation experience for spot-checking small datasets.

Automated evaluation is useful for measuring quality and safety at scale with increased coverage to provide more comprehensive results. Automated evaluation tools also enable ongoing evaluations that periodically run to monitor for regression as the system, usage, and mitigations evolve. We support two main methods for automated evaluation of generative AI applications: traditional machine learning evaluations and AI-assisted evaluation.

![Evaluations](images/lab3grab31.png)

We recommend clicking on the link `Learn more about metrics` at the top of the evaluation page to understand the metrics used in the evaluation.

![Evaluation Metrics](images/lab3grab32.png)


## Solution

<details markdown="block">
<summary>Expand this section to view the solution</summary>

##### 1) Evaluate your Chat flow

1. Go to your browser and type: [https://ai.azure.com](https://ai.azure.com)
   
2. Select the project created earlier and choose the **Prompt flow** item in the **Tools** section of the **Build** tab.

###### 1.1) Prepare you chat flow for evaluation

For the RAG flow that you created earlier to be evaluated, you must include additional information to the output node of this flow, specifically the context used to generate the answer.

This information will be used by the Evaluation Flow. To do this, just follow these steps:

1. In the Flows section of **Prompt Flow**, open the `Multi-Round Q&A on Your Data` flow that you created in the previous exercise. This will be the flow we use for evaluation.

![LLMOps Workshop](images/lab3grab1.png)

2. Create a new output named `documents` in the Outputs node. This output will represent the documents that were retrieved in the `lookup` node and subsequently formatted in the `generate_prompt_context` node.

3. Assign the output of the `generate_prompt_context` node to the `documents` output, as shown in the image below.

![LLMOps Workshop](images/lab3grab2.png)

4. Click **Save** before moving to the next section.

###### 1.2) Create your evaluation flows

1. Still in the **Prompt flow** item in the **Tools** section of the **Build** tab, click on the blue **Create** button.

![LLMOps Workshop](images/lab3grab3.png)

2. Select the **Evaluation Flow** filter and click on **Clone** on the **QnA Groundedness Evaluation** card.

![LLMOps Workshop](images/lab3grab4.png)

3. Click on the other **Clone** button to create a copy of the flow.

![LLMOps Workshop](images/lab3grab5.png)

4. A flow will be created with the following structure:

![LLMOps Workshop](images/lab3grab6.png)

5. Update the `Connection` field to point to a gpt-4 deployment in `groundedness_score` node also update max_tokens to `1000` as shown in the next figure.  
   
![LLMOps Workshop](images/lab3grab7.png)

6. After updating the connection information, click on **Save** in the evaluation flow and navigate to the Flows section in **Prompt Flow** item.

7. Now, you will repeat the same steps described so far in this **section 1.2** to create **two** additional evaluation flows, one `QnA Relevance Evaluation` and another `QnA GPT Similarity Evaluation`. The two images below show where these flows are in the prompt flow gallery.

> You will repeat **section 1.2** steps twice since you will need to create two additional evaluation flows.

> Note that the LLM nodes, where you will set the Azure OpenAI connection for each flow, have slightly different names: **relevance_score** and **similarity_score**, respectively.

QnA Relevance Evaluation:

![LLMOps Workshop](images/lab3grab8.png)


QnA GPT Similarity Evaluation:

![LLMOps Workshop](images/lab3grab9.png)


###### 1.3) Run the evaluation

In the Flows section of **Prompt Flow**, open the `Multi-Round Q&A on Your Data` flow that you created in the previous exercise. This will be the flow we use for evaluation.

1. Start the automatic runtime by selecting **Start compute session**. The runtime will be useful for you to work with the flow moving forward.

![LLMOps Workshop](images/lab3grab10.png)

2. Now select the **Custom evaluation** option in the Evaluate menu.

![LLMOps Workshop](images/lab3grab11.png)

3. In the `Prompt_variants` option, select the option to run only **two variants** to avoid reaching your GPT-4 model quota limit, as shown in the example image below, click **Next**.

![LLMOps Workshop](images/lab3grab12.png)

4. Select **Add new data**.

![LLMOps Workshop](images/lab3grab13.png)

5. Upload the file [data.csv](files/data.csv) inside the lesson_03 folder.

![LLMOps Workshop](images/lab3grab14.png)

6. After clicking on **Add**  proceed to map the input fields as shown below: 

![LLMOps Workshop](images/lab3grab15.png)

7. Select the three evaluation flows you just created.

![LLMOps Workshop](images/lab3grab16.png)

8. Great job so far! Now, let's move on to the next step. Click on **Next** to set up the `question`, `context`, `ground_truth` and `answer` fields for each evaluation flow. You can see how to do this in the three images below.

> **Note:** Please take a moment to ensure you've selected the correct value. It's crucial for accurate metric calculation. Notice that the default values initially presented in the wizard are not the same as those indicated in the following images. Keep up the good work!

**QnA GPT Similarity Evaluation**

![LLMOps Workshop](images/lab3grab17.png)

**QnA Groundedness Evaluation**

![LLMOps Workshop](images/lab3grab18.png)

**QnA Relevance Evaluation**

![LLMOps Workshop](images/lab3grab19.png)

Click on **Submit** to start the evaluation.

![LLMOps Workshop](images/lab3grab20.png)

9. The evaluation process has started. To view all evaluations (one per variant), please navigate to the **Evaluation** section under the **Build** tab (If you see not started they are waiting on resources, they will say completed when finished).

![LLMOps Workshop](images/lab3grab21.png)

10. Upon selecting specific evaluation results, you will have the ability to view their detailed information.

11. You can also select **Switch to dashboard view** to access a dashboard that provides a tabular and visual comparison between the rounds of different variations, as shown in the following images.

*Table comparison*

![LLMOps Workshop](images/lab3grab22.png)

*Chart comparison*

![LLMOps Workshop](images/lab3grab23.png)

</details>
