---
title: '1. Project Setup'
layout: default
nav_order: 1
parent: 'Exercise 05: Automate Everything'
---

# Task 01 - Project Setup

## Description

In this exercise, we will learn how to setup **LLMOps with Prompt Flow** template to deploy an end-to-end LLMOps solution using **Prompt Flow** and **Github Actions**.

## Success Criteria

* Check and set up prerequisites
* Create a Service principal
* Setup a GitHub repository
* Create Azure OpenAI model deployment
* Update template configuration parameters

## Pre-requisites

Please ensure that you have the following resources in order to proceed with the hands-on steps:

* An Azure subscription.
* An Azure AI Project and AI Resource (created in a previous lab).
* A GitHub account.
* A workstation with the following options:
  * Local machine or VM equipped with the following tools:
    * Git (usually pre-installed on most systems)
    * Python 3.10: [Download Python](https://www.python.org/downloads/)
    * VS Code: [Download VS Code](https://code.visualstudio.com/download)
    * Azure CLI: [Install Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli)
  
**Important:** Since we will be using bash commands, the operating system must be Windows with WSL, Linux, or MacOS.

## References

* [LLMOps with Prompt Flow and GitHub](https://learn.microsoft.com/azure/machine-learning/prompt-flow/how-to-end-to-end-llmops-with-prompt-flow)
* [LLMOps with Prompt Flow template repo](https://github.com/microsoft/llmops-promptflow-template)
* [Basic LLMOps example](https://github.com/Azure/llmops-gha-demo/blob/main/docs/e2e_llmops_with_promptflow.md)

## Solution

<details markdown="block">
<summary>Expand this section to view the solution</summary>

##### 1) Check and set-up pre-requisites

1. Login to Azure so you can execute the commands for the following steps.

    ```bash
    az login
    ```

2. Create a Service Principle.

    An Azure service principal (SP) is a special type of identity that can be used by automated tools to access Azure resources.
    We will use a service principal to grant GitHub Actions the permission to use the resources in our Azure subscription.

    Run the following bash script after updating the `<subscription_id>` placeholder with your subscription id.

    Note: The service principal name will be automatically generated in the format: LLMOps-(seconds since epoch). If you prefer a different name, you can specify it in the `spname` variable.

    ```bash
    subscriptionId="<subscription_id>"
    
    spname="LLMOps-$(date +%s)"
    roleName="Owner"
    servicePrincipalName="Azure-ARM-${spname}"
    
    echo "Using subscription ID $subscriptionId"
    echo "> Setting subscription id"
    az account set --subscription $subscriptionId
    
    echo "Creating SP for RBAC with name $servicePrincipalName, with role $roleName and in scopes /subscriptions/$subscriptionId"
    az ad sp create-for-rbac --name $servicePrincipalName --role $roleName --scopes /subscriptions/$subscriptionId --sdk-auth
    
    echo "Please ensure that the information created here is properly saved for future use."
    ```

After executing the script, you will receive the following information regarding the service principal.

![A JSON file is displayed that shows the clientID and clientSecret properties in addition to others.](images/lab6grab1.png)

This is the information to be used for GitHub actions to authenticate in Azure.
**Save it in a secure place**, as you will need it later for GitHub configuration.

##### 2) Set up GitHub Repository

1. Go to GitHub and create a new **private** repository.

    ![A GitHub dashboard is displayed with the New button highlighted for creating a new repository.](images/lab6grab2.png)

    Let's use **llmops-project** as the repo name for this workshop, if you chose a different repo name, you can replace **llmops-project** with the name you have chosen whenever you see it in the lab commands.

    ![The new repository form is displayed with llmops-project shown as the repository name.](images/lab6grab3.png)

2. Create a GitHub personal access token.

    Now you will create a Github personal access token to work with your repository from this lab.

    Go to your GitHub account settings by selecting your profile photo and then selecting Settings. Execute the following steps:

    1. In the left sidebar, select Developer settings.
    2. In the left sidebar, select Personal access tokens (Fine-grainded tokens).
    3. Select **Generate new token**.

        ![The Generate new token button is highlighted.](images/lab6grab4.png)

    4. Choose the repo you just created in the Repository access section.

        ![The Only select repositories radio button is selected and the new repo is selected in the dropdown list.](images/lab6grab4b.png)

    5. In the Permissions section, add **Read and write** to the following items: **Administration**, **Contents**, and **Workflows** (Metadata is mandatory).

        ![The selected permissions are displayed within the overview grid.](images/lab6grab4c.png)

    6. Select **Generate token** and _copy the token_ to use in the next command.

3. Populate your repo with the LLMOps project template.

    Now you will populate your new repo with the LLMOps Prompt Flow Template Repo.

    In order to do that, run the following bash script, after update the `<github_org>` with your org name and `<github_pat>` with the token created above.

    ```bash
    github_org=<github_org>
    github_repo=llmops-project
    github_pat=<github_pat>
    
    echo "> Cloning your project repository"
    git clone https://${github_pat}@github.com/${github_org}/${github_repo}.git
    
    cd $github_repo
    
    echo "> Adding a new remote named 'original'"
    git remote add original https://github.com/microsoft/llmops-promptflow-template.git
    
    echo "> Fetching from 'original'"
    git fetch original main
    
    echo "> Merging with 'original/main'"
    git merge original/main
    
    echo "> Pushing to 'origin main'"
    git push origin main
    ```

    > **Note:** If the last command (`git push origin main`) fails, try executing `git push` instead.

4. Create a development branch and set it as default.

    The project template assumes that the team utilizes the development branch as the primary source for coding and enhancing the prompt quality.

    Create a development branch by branching off from the main branch and designate it as the default branch to ensure that all pull requests are directed towards it.
    To generate the development branch and set it as the default in the **local repository**, execute the following bash cell.

    ```bash
    echo "> Creating a development branch"
    git checkout -b development
    
    echo "> Pushing the development branch to the remote repository"
    git push origin development
    ```

    To make development branch as default in the GitHub remote repo, go to your GitHub repository on the web and execute the following steps:

    1. Select the **Settings** tab.
    2. In the **Default branch** section, select the **switch to another branch** (double-arrow) icon to edit the default branch selection.
    3. Select the branch you want to set as the default branch (development) from the dropdown list.
    4. Select **Update** to save your changes.

    ![The general settings for the repo are displayed with the default branch section highlighted.](images/lab6grab5.png)

5. Set up authentication with Azure and Github.

    To start, go to the **Settings** tab of your GitHub project. Then, navigate to **Secrets and variables** followed by **Actions** and select **New repository secret**.

    ![The new repository secret button is highlighted.](images/lab6grab6.png)

    Now, create a repository secret on GitHub called `AZURE_CREDENTIALS` and provide the Azure Service Principal details you saved in an earlier step as its content. Select **Add secret** to save the secret.

    ![The new secret form is displayed.](images/lab6grab7.png)

##### 3) Create Azure OpenAI model deployment

1. Create model deployment.

    The example flows in the LLMOps template utilize a deployment of the Azure OpenAI model called gpt-4.

    Please use Azure OpenAI Studio at https://ai.azure.com/ to create a deployment named `gpt-4` in the AI Project you set up during step 1 of this hands-on exercise.

    To do this, complete the following steps:

    1. Go to the **Build** tab and select **Deployments** in the left-hand menu.
    2. Select the **+ Create** button under Deployments.

        ![The Create button is highlighted on the Deployments page.](images/lab6grab8.png)

    3. Select the model you wish to deploy (gpt-35-turbo or gpt-4, depending on what is available in your region) and select **Confirm**.

    4. Select the **Deploy** button to deploy the selected model.

          ![The Deploy button is highlighted.](images/lab6grab10.png)

2. Update Azure OpenAI connection name.

    LLMOps with Prompt flow project template example flows use an Azure OpenAI connection named `aoai`, in this hands on we will use AI Project's default Azure OpenAI connection.

    Run the next code block to update `flow.dag.yaml` for both standard and post-production-evaluation flows of the `named_entity_recognition` example to use the AI Project's default Azure OpenAI connection.

    The flow definition file for both `named_entity_recognition` standard and `post-production-evolution` flows can be found inside the template project folder in:

    * `named_entity_recognition\flows\standard\flow.dag.yaml`
    * `named_entity_recognition\flows\post-production-evaluation\flow.dag.yaml`

    **Create a new Python script** named `update_flow_connection.py` and paste the following code into it. Make sure you update the `github_repo` and `model_deployment_name` variables with the correct values:

    ```python
    import fileinput
    import sys
    
    github_repo="llmops-project"
    model_deployment_name="gpt-4"
    
    def modify_file(filename):
        with fileinput.FileInput(filename, inplace=True) as file:
            for line in file:
                line = line.replace('connection: aoai', 'connection: Default_AzureOpenAI')
                line = line.replace('deployment_name: gpt-35-turbo', f'deployment_name: {model_deployment_name}')
                sys.stdout.write(line)
    
    filenames = [f'{github_repo}/named_entity_recognition/flows/standard/flow.dag.yaml',
                    f'{github_repo}/named_entity_recognition/flows/post-production-evaluation/flow.dag.yaml']
    
    list(map(modify_file, filenames))
    ```

##### 4) Update Template Configuration Parameters

The **LLMOps with Prompt Flow** template repo include three example use cases, we will focus on the **named_entity_recognition** one.
In this step, you will configure the name_entity_recognition flow's LLMOps and deployment parameters. After that we will be able
to create a pull request for this example.

1. Update flow's LLMOps parameters.

    The file **llmops_config.json** contains information related to the resources and directories used in each environment.

    1. Navigate to the **named_entity_recognition** folder and open the **llmops_config.json** file, note that the parameters KEYVAULT_NAME, RESOURCE_GROUP_NAME, and WORKSPACE_NAME will be blank.

    2. Update the files respectively with the name of the key vault, resource group, and AI project name created in step 1 of this lab.

    For simplicity, we will use the same resources for all environments.

    Example configuration:

    ![An example of the llmops_config.json file is displayed.](images/lab6grab11.png)

    The following shows the configuration parameters for the **llmops_config.json** file:

    * `ENV_NAME`:  This represents the environment type. (The template example supports *pr*, *dev* and *postprodeval* environments.)
    * `RUNTIME_NAME`:  This is the name of a Prompt Flow runtime environment, used for executing the prompt flows. Add values to this field only when you are using dedicated runtime and compute. The template uses automatic runtime by default.
    * `KEYVAULT_NAME`:  This points to an Azure Key Vault related to the Azure ML service, a service for securely storing and managing secrets, keys, and certificates.
    * `RESOURCE_GROUP_NAME`:  Name of the Azure resource group related to Azure ML workspace.
    * `WORKSPACE_NAME`:  This is name of Azure ML workspace.
    * `STANDARD_FLOW_PATH`:  This is the relative folder path to files related to a standard flow. e.g.  e.g. "flows/standard_flow.yml"
    * `EVALUATION_FLOW_PATH`:  This is a string value referring to relative evaluation flow paths. It can have multiple comma separated values- one for each evaluation flow. e.g. "flows/eval_flow_1.yml,flows/eval_flow_2.yml"

2. Update flow's deployment parameters.

    The file **deployment_config.json** contains information related to the solution deployment.

    1. Navigate to the **named_entity_recognition** folder and open the **configs/deployment_config.json** file.

    2. Update the ENDPOINT_NAME and CURRENT_DEPLOYMENT_NAME parameters.

    3. Modify the configuration values in the **deployment_config.json** file in the azure_managed_endpoint section for the **dev** environment.

    During this lab we'll only use the dev environment, so no need to worry about the other environments.

    Example configuration:

    ![An example of the deployment_config.json file is displayed.](images/lab6grab12.png)

    The following shows the configuration parameters for the **deployment_config.json** file:

    * `ENV_NAME`: This indicates the environment name, referring to the "dev" or "prod" or any other environment where the prompt will be deployed and used in real-world scenarios.
    * `TEST_FILE_PATH`: The value represents the file path containing sample input used for testing the deployed model.
    * `ENDPOINT_NAME`: The value represents the name or identifier of the deployed endpoint for the prompt flow.
    * `ENDPOINT_DESC`: It provides a description of the endpoint. It describes the purpose of the endpoint, which is to serve a prompt flow online.
    * `DEPLOYMENT_DESC`: It provides a description of the deployment itself.
    * `PRIOR_DEPLOYMENT_NAME`: The name of prior deployment. Used during A/B deployment. The value is "" if there is only a single deployment. Refer to CURRENT_DEPLOYMENT_NAME property for the first deployment.
    * `PRIOR_DEPLOYMENT_TRAFFIC_ALLOCATION`:  The traffic allocation of prior deployment. Used during A/B deployment. The value is "" if there is only a single deployment. Refer to CURRENT_DEPLOYMENT_TRAFFIC_ALLOCATION property for the first deployment.
    * `CURRENT_DEPLOYMENT_NAME`: The name of current deployment.
    * `CURRENT_DEPLOYMENT_TRAFFIC_ALLOCATION`: The traffic allocation of current deployment. A value of 100 indicates that all traffic is directed to this deployment.
    * `DEPLOYMENT_VM_SIZE`: This parameter specifies the size or configuration of the virtual machine instances used for the deployment.
    * `DEPLOYMENT_BASE_IMAGE_NAME`: This parameter represents the name of the base image used for creating the Prompt Flow runtime.
    * `DEPLOYMENT_CONDA_PATH`: This parameter specifies the path to a Conda environment configuration file (usually named conda.yml), which is used to set up the deployment environment.
    * `DEPLOYMENT_INSTANCE_COUNT`:This parameter specifies the number of instances (virtual machines) that should be deployed for this particular configuration.
    * `ENVIRONMENT_VARIABLES`: This parameter represents a set of environment variables that can be passed to the deployment.

3. Push updates to remote repo.

    To update the remote repository with the newly modified configurations, simply update the project repo name in **<github_repository>** and then push to the **development** branch by executing the next cell.

    ```bash
    cd llmops-project
    git add .
    git commit -m "configuration update"
    git push origin development
    ```

</details>
